{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_32dmjb9cJL_",
        "outputId": "bfae4a2e-76ba-4ff7-b3a3-ac0df2e6ffd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the datasets from the provided paths in Google Colab\n",
        "train_df = pd.read_csv('/content/Corona_NLP_train.csv', encoding='latin1')\n",
        "test_df = pd.read_csv('/content/Corona_NLP_test.csv', encoding='latin1')\n",
        "\n",
        "# Display the first few rows of the training data to check\n",
        "train_df.head()\n",
        "# Select a subset of 100 samples from the training dataset\n",
        "train_df = train_df.iloc[:100] # Use .iloc to select rows by position"
      ],
      "metadata": {
        "id": "y48DCVCwdykr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# To load the sentiment analysis pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# To test the model with multiple reviews from the subset\n",
        "results = []\n",
        "for i in range(10):  # Testing the first 10 reviews\n",
        "    review = train_df.iloc[i][\"OriginalTweet\"] # Access the 'OriginalTweet' column using .iloc for position-based indexing\n",
        "    sentiment = classifier(review)\n",
        "    results.append((review, sentiment))\n",
        "\n",
        "# To show the results\n",
        "for i, (review, sentiment) in enumerate(results):\n",
        "    print(f\"Review {i+1}: {review}\")\n",
        "    print(f\"Sentiment: {sentiment}\")\n",
        "    print(\"-\" * 40)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NxPcTlktDjJ9",
        "outputId": "b5a81d4a-ba13-488f-eb62-1135eddc620a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 1: @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8\n",
            "Sentiment: [{'label': 'NEGATIVE', 'score': 0.9927721619606018}]\n",
            "----------------------------------------\n",
            "Review 2: advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order\n",
            "Sentiment: [{'label': 'NEGATIVE', 'score': 0.9970411658287048}]\n",
            "----------------------------------------\n",
            "Review 3: Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P\n",
            "Sentiment: [{'label': 'NEGATIVE', 'score': 0.9947763681411743}]\n",
            "----------------------------------------\n",
            "Review 4: My food stock is not the only one which is empty...\r\r\n",
            "\r\r\n",
            "PLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \r\r\n",
            "Stay calm, stay safe.\r\r\n",
            "\r\r\n",
            "#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j\n",
            "Sentiment: [{'label': 'NEGATIVE', 'score': 0.9970644116401672}]\n",
            "----------------------------------------\n",
            "Review 5: Me, ready to go at supermarket during the #COVID19 outbreak.\r\r\n",
            "\r\r\n",
            "Not because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\r\r\n",
            "\r\r\n",
            "#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n\n",
            "Sentiment: [{'label': 'NEGATIVE', 'score': 0.999160885810852}]\n",
            "----------------------------------------\n",
            "Review 6: As news of the regionÂs first confirmed COVID-19 case came out of Sullivan County last week, people flocked to area stores to purchase cleaning supplies, hand sanitizer, food, toilet paper and other goods, @Tim_Dodson reports https://t.co/cfXch7a2lU\n",
            "Sentiment: [{'label': 'POSITIVE', 'score': 0.6315158605575562}]\n",
            "----------------------------------------\n",
            "Review 7: Cashier at grocery store was sharing his insights on #Covid_19 To prove his credibility he commented \"I'm in Civics class so I know what I'm talking about\". https://t.co/ieFDNeHgDO\n",
            "Sentiment: [{'label': 'NEGATIVE', 'score': 0.8098400831222534}]\n",
            "----------------------------------------\n",
            "Review 8: Was at the supermarket today. Didn't buy toilet paper. #Rebel\r\r\n",
            "\r\r\n",
            "#toiletpapercrisis #covid_19 https://t.co/eVXkQLIdAZ\n",
            "Sentiment: [{'label': 'NEGATIVE', 'score': 0.9985405206680298}]\n",
            "----------------------------------------\n",
            "Review 9: Due to COVID-19 our retail store and classroom in Atlanta will not be open for walk-in business or classes for the next two weeks, beginning Monday, March 16.  We will continue to process online and phone orders as normal! Thank you for your understanding! https://t.co/kw91zJ5O5i\n",
            "Sentiment: [{'label': 'NEGATIVE', 'score': 0.9968546628952026}]\n",
            "----------------------------------------\n",
            "Review 10: For corona prevention,we should stop to buy things with the cash and should use online payment methods because corona can spread through the notes. Also we should prefer online shopping from our home. It's time to fight against COVID 19?. #govindia #IndiaFightsCorona\n",
            "Sentiment: [{'label': 'NEGATIVE', 'score': 0.9968441724777222}]\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the specified BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rEXdOPXXfx7R"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "source": [
        "import torch\n",
        "\n",
        "# Get embeddings from BERT\n",
        "def get_bert_embeddings(tokens):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "# Tokenize the training data (assuming 'train_df' contains your training data)\n",
        "X_train_texts = train_df['OriginalTweet'].tolist() # Extract the text column\n",
        "X_train_tokens = tokenizer(X_train_texts, truncation=True, padding=True, return_tensors='pt') # Tokenize\n",
        "\n",
        "X_train_bert = get_bert_embeddings(X_train_tokens)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-7_DYTP7EDhF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install sklearn\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPQubDDVcRtU",
        "outputId": "6ca6cfc8-b0f6-4eaa-d800-ef86cd6bc8ce"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data: only keep the text and sentiment labels\n",
        "train_df = train_df[['OriginalTweet', 'Sentiment']] # Keep only relevant columns\n",
        "\n",
        "# Convert sentiment labels to binary for simplicity (positive/negative)\n",
        "train_df['Sentiment'] = train_df['Sentiment'].apply(lambda x: 1 if x == 'Positive' else 0)\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9P3VbXVcZsL",
        "outputId": "a34efc4d-4ceb-41fa-92b7-7c77e2ab3adc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-299795eb789d>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['Sentiment'] = train_df['Sentiment'].apply(lambda x: 1 if x == 'Positive' else 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and load pre-trained GloVe embeddings (e.g., GloVe 50d)\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "glove_model = {}\n",
        "with open(\"glove.6B.50d.txt\", \"r\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.array(values[1:], dtype=\"float32\")\n",
        "        glove_model[word] = vector\n",
        "\n",
        "def get_glove_embeddings(texts):\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        words = text.split()\n",
        "        word_vectors = [glove_model[word] for word in words if word in glove_model]\n",
        "        if word_vectors:\n",
        "            embeddings.append(np.mean(word_vectors, axis=0))\n",
        "        else:\n",
        "            embeddings.append(np.zeros(50))  # Adjust for 50 dimensions\n",
        "    return np.array(embeddings)\n",
        "\n",
        "X_train_glove = get_glove_embeddings(train_df['OriginalTweet'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "i13HcCPTdQCr",
        "outputId": "15b17198-9143-4ba3-9d4e-27ef41431f7e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-31 22:01:21--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-08-31 22:01:21--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-08-31 22:01:21--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2024-08-31 22:04:01 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-9968e3a3976d>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mglove_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "glove_model = {}\n",
        "with open(\"glove.6B.50d.txt\", \"r\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.array(values[1:], dtype=\"float32\")  # Now np is defined\n",
        "        glove_model[word] = vector\n",
        "\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "glove_model = {}\n",
        "with open(\"glove.6B.50d.txt\", \"r\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.array(values[1:], dtype=\"float32\")\n",
        "        glove_model[word] = vector\n",
        "\n",
        "def get_glove_embeddings(texts):\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        words = text.split()\n",
        "        word_vectors = [glove_model[word] for word in words if word in glove_model]\n",
        "        if word_vectors:\n",
        "            embeddings.append(np.mean(word_vectors, axis=0))\n",
        "        else:\n",
        "            embeddings.append(np.zeros(50))  # Adjust for 50 dimensions\n",
        "    return np.array(embeddings)\n",
        "\n",
        "X_train_glove = get_glove_embeddings(train_df['OriginalTweet'])\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i-ll2x0f8aC",
        "outputId": "93a1483d-8c16-4f24-f263-5616142bf3bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-31 22:14:03--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-08-31 22:14:03--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-08-31 22:14:04--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  5.16MB/s    in 2m 49s  \n",
            "\n",
            "2024-08-31 22:16:53 (4.85 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_bert, train_df['Sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate models\n",
        "def evaluate_embeddings(X_train, X_val, y_train, y_val):\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    precision = precision_score(y_val, y_pred)\n",
        "    recall = recall_score(y_val, y_pred)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Evaluate BERT embeddings\n",
        "bert_metrics = evaluate_embeddings(X_train_bert, X_train_bert, y_train, y_val)\n",
        "print(f\"BERT - Accuracy: {bert_metrics[0]}, Precision: {bert_metrics[1]}, Recall: {bert_metrics[2]}, F1-Score: {bert_metrics[3]}\")\n",
        "\n",
        "# Evaluate Word2Vec embeddings\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_word2vec, train_df['Sentiment'], test_size=0.2, random_state=42)\n",
        "word2vec_metrics = evaluate_embeddings(X_train, X_val, y_train, y_val)\n",
        "print(f\"Word2Vec - Accuracy: {word2vec_metrics[0]}, Precision: {word2vec_metrics[1]}, Recall: {word2vec_metrics[2]}, F1-Score: {word2vec_metrics[3]}\")\n",
        "\n",
        "# Evaluate GloVe embeddings\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_glove, train_df['Sentiment'], test_size=0.2, random_state=42)\n",
        "glove_metrics = evaluate_embeddings(X_train, X_val, y_train, y_val)\n",
        "print(f\"GloVe - Accuracy: {glove_metrics[0]}, Precision: {glove_metrics[1]}, Recall: {glove_metrics[2]}, F1-Score: {glove_metrics[3]}\")\n"
      ],
      "metadata": {
        "id": "s5EGRMc7hKu7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}